#CH1
房屋价格的函数x，y
采用了Regression，数值是连续的

肿瘤良性问题 归类为01分类问题，数据本身为离散
简单介绍SVM，处理无限多维feature
鸡尾酒舞会，两个人，两个麦克风如何分辨两个人不同的声音
用的是ICA技术，independent component analysis
强化学习算法，reinforcement learning  目的是做出一系列的决策

#Ch2
linear Regression
gradient descent
normal equation

汽车自动驾驶视频，Ng称其为一个regression问题，因为方向盘是一个连续性的预测动作
Portland  房价预测回归问题
hypothesis $ H(X)=H_θ(X)=θ_0+θ_1X_1$
$ H(X)= \sum_{i=0}^{n}\Theta_iX_i=\Theta^TX$
Gradient descent/ batch/随机/mini
通常普通的J（θ）只是一个二次方程，它的解的图像是一个碗形状，它的等高线画出来是个椭圆形
最终目的是不断趋于收敛
我们用梯度下降法来解最小二乘的解
也可以用线性代数直接求解，但是没有数值解来的快。
线性回归中，不断摆动的线，就是收敛的过程。

#Ch3
linear regression
locally weighted regression LWR
probability interpretation
logistic regression
perception 感知机
newton method
$J(\theta)=1/2\sum_{i=1}^{m}(H_\theta(x^{(i)})-y^{(i)})^2$

$\theta=(X^TX)^{-1}X^TY$

过拟合与欠拟合
比如使用多项式函数，或许可以非常完美的拟合所有点，但是它不是一个好的预测函数
linear regression 是一种参数学习方法，因为θ是fixed
非参数学习算法：the number of parameters goes with m , m is training set
就是参数随着样本多少的变化而变化
LWR就是一种非参数学习算法它也叫Loess，对于数据量特别大，效果不好
线性模型的概率解释，为什么用最小二乘法，不是用绝对值，四次方等等来衡量最优距离
证明思路：先设定误差服从正态分布有IID性质，然后P（y-θx）也服从正态，再引入最大似然法.
通过最大似然法得出，想要L（θ）出现的概率最大，就要minimize J（θ）也就是最小二乘法
分类问题：sigmoid function or logistic function
why choose sigmoid function 有几个重要理由，下周再说


#Ch4

牛顿法

#前四章总结
我们有数据集（x,y）,无论是预测还是分类，首先我们需要用一个函数H（X）来接近y，y和H（X）肯定不完全一致，衡量它们的区别距离就是cost function J(θ)，选择J（θ），当J（θ）是线性的时候，还可以用矩阵来解开，但是多元，或者非线性就解不开了。需要用迭代的数值算法，GD法，牛顿法之类的，牛顿法和拟牛顿法比较快，但是需要计算海森矩阵H，而且非凸情况下还可能不收敛。GD在神经网络中应用的更好。SGD在深度学习收青睐。
当x是线性的时候有GLM，非线性时候有svm和神经网络

#Ch5
Generate learning algorithm生成学习算法
GDA高斯判别分析
两者对比
naive bayes 朴素贝叶斯
Laplace smooth 拉普拉斯平滑

discriminative判别分析
学习p(y|x ) 或者学习h(x)属于{0，1}，逻辑回归是判别算法的一个例子

高斯判别分析法
当x|y 服从高斯分布，可以推导出logistic posterior for p(y=1|x)是一个logistic function
反推并不成立
继续：if x|y=0 服从 poission（λ_0）,x|y=1 服从 poission（λ_1）
那么推出 p(y=1|x)是一个logistic function
x|y 服从高斯分布是一个很强的假设，一旦你发现数据有这种趋势，做出来这种假设，它比逻辑回归好用的多。
生成学习算法只要很少的数据就可以做出不错的结果，逻辑回归假设更少，但是需要更多的数据，而且假设更少，模型更robust
naive bayes也是生成算法
在前面我们谈论到的算法都是在给定x的情况下直接对p(y|x;Θ)进行建模。例如，逻辑回归利用hθ(x) = g(θTx)对p(y|x;Θ)建模。

现在考虑这样一个分类问题，我们想根据一些特征来区别动物是大象(y=1)还是狗(y=0)。给定了这样一个训练集，逻辑回归或感知机算法要做的就是去找到一个决策边界，将大象和狗的样本分开来。但是如果换个思路，首先根据大象的特征来学习出一个大象的模型，然后根据狗的特征学习出狗的模型，最后对于一个新的样本，提取它的特征先放到大象的模型中求得是大象的概率，然后放到狗的模型中求得是狗的概率，最后我们比较两个概率哪个大，即确定这个动物是哪种类型。也即求p(x|y)(也包括p(y))，y为输出结果，x为特征。

判别学习算法（discriminative learning algorithm）：直接学习p(y|x)或者是从输入直接映射到输出的方法

生成学习算法（generative learning algorithm）：对p(x|y)(也包括p(y))进行建模。
