#CH1
房屋价格的函数x，y
采用了Regression，数值是连续的

肿瘤良性问题 归类为01分类问题，数据本身为离散
简单介绍SVM，处理无限多维feature
鸡尾酒舞会，两个人，两个麦克风如何分辨两个人不同的声音
用的是ICA技术，independent component analysis
强化学习算法，reinforcement learning  目的是做出一系列的决策

#Ch2
linear Regression
gradient descent
normal equation

汽车自动驾驶视频，Ng称其为一个regression问题，因为方向盘是一个连续性的预测动作
Portland  房价预测回归问题
hypothesis $ H(X)=H_θ(X)=θ_0+θ_1X_1$
$ H(X)= \sum_{i=0}^{n}\Theta_iX_i=\Theta^TX$
Gradient descent/ batch/随机/mini
通常普通的J（θ）只是一个二次方程，它的解的图像是一个碗形状，它的等高线画出来是个椭圆形
最终目的是不断趋于收敛
我们用梯度下降法来解最小二乘的解
也可以用线性代数直接求解，但是没有数值解来的快。
线性回归中，不断摆动的线，就是收敛的过程。

#Ch3
linear regression
locally weighted regression LWR
probability interpretation
logistic regression
perception 感知机
newton method
$J(\theta)=1/2\sum_{i=1}^{m}(H_\theta(x^{(i)})-y^{(i)})^2$

$\theta=(X^TX)^{-1}X^TY$

过拟合与欠拟合
比如使用多项式函数，或许可以非常完美的拟合所有点，但是它不是一个好的预测函数
linear regression 是一种参数学习方法，因为θ是fixed
非参数学习算法：the number of parameters goes with m , m is training set
就是参数随着样本多少的变化而变化
LWR就是一种非参数学习算法它也叫Loess，对于数据量特别大，效果不好
线性模型的概率解释，为什么用最小二乘法，不是用绝对值，四次方等等来衡量最优距离
证明思路：先设定误差服从正态分布有IID性质，然后P（y-θx）也服从正态，再引入最大似然法.
通过最大似然法得出，想要L（θ）出现的概率最大，就要minimize J（θ）也就是最小二乘法
分类问题：sigmoid function or logistic function
why choose sigmoid function 有几个重要理由，下周再说


#Ch4

牛顿法

#前四章总结
我们有数据集（x,y）,无论是预测还是分类，首先我们需要用一个函数H（X）来接近y，y和H（X）肯定不完全一致，衡量它们的区别距离就是cost function J(θ)，选择J（θ）是最小二乘法，可以用MLE来证明这个过程。当J（θ）是线性的时候，还可以用矩阵来解开，但是多元，或者非线性就解不开了。需要用迭代的数值算法，GD法，牛顿法之类的，牛顿法比较快，但是需要计算海森矩阵H，而且非凸情况下还可能不收敛。GD在神经网络中应用的更好。
当x是线性的时候有GLM，非线性时候有svm和神经网络
