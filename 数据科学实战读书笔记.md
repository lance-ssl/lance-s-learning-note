#CH3
##**两大类问题：预测和分类**
模型和算法很难区分的非常清楚，比如kmeans既是算法，又是一种特殊高斯混合模型
输出变量和预测变量是线性的是一个非常大胆的假设，也是一个最简单的假设，而有时候确认变量间的线性关系是非常困难的，从微积分的角度来说，只要函数是连续的，就可以被一些局部线性的函数所拟合，因此局部线性假设大多数情况下都合理，但是全局性的线性假设就很难说了。

模型对于数据来说，主要是捕捉其中两方面的信息：trend趋势，variation变动幅度
做一组数据的**线性拟合**，对于线性回归来说，最优的只有一条，这里最优指的是距离最小化，距离是指“离差平方和”，这种估计办法就是最小二乘法。
**离差平方和** 表示为RSS（Residual Sum of Squares），对其求导，令其为0，得到$\hat{β}$，代表β的估计值，而真实值无从得知。
输入一个x，得到模型的结果y，我们有多大自信认为它接近真实值呢，叫置信区间。
为了增加y的不确定性，引入e，噪声误差项，该残差服从一个均值为0，方差未知的正态分布。（iid）
其实可以从数学方向证明，无论误差项怎么分布，最小二乘都具有同样的性质，它是无偏估计量，并且具有最小的估计方差。

**模型评估标准**： R方 数据中能够被模型解释的方差占数据总方差的比重
P值 ：一个原假设，假设参数值β为0，对于每一个β，p值代表的是在原假设基础上我们可以得到观测数据的概率。
交叉验证，28法则
均方误差（mean squared error）是损失函数的一种，是线性回归中最常用的误差指标，如果假设误差项是正态分布，模型的估计还可以完全依赖最大似然函数法。
在到底使用哪些预测变量时候，散点图是个很好的工具，可以画出预测变量和每一个自变量的散点图，以及自变量之间的散点图。

**变换**：
可以使用x的高阶项，可以对变量做适当变换，来满足线性回归模型的线性要求


**分类是建模的基本问题 KNN（有监督方法）**
在一个给定做好分类的数据集中，给未来的没分类的数据分类
对于分类问题可以用线性回归模型么？ 答案是不一定!
线性模型的输出是连续值，但是分类是离散的，线性模型想要得出结果，就得用阀值来把结果离散化。
KNN的核心想法是根据属性值的相似度找到某个对象的相似对象，并且让相似对象投票。
难点在于选择一个最优k值
#####详细步骤：
1. 确定相似性定义，通常叫做距离
2. 分割训练数据和测试数据
3. 选择一个模型评价标准（通常误分率是一个好选择）
4. 选择不同k值，选出最优K值

相似性/距离测度
变量取值要在大致一个水平上，有多种距离可以选择，比如曼哈顿距离，jaccard距离。。。。

模型评价标准 ：是敏感度（sensitivity）和特异度（specificity）的权衡，看模型实际需求

k值选个奇数有利于投票

KNN是非参数办法，也就是说对于数据的生成过程和数据的分布没有任何假设，也没有任何需要估计的参数。

**K—Means方法**
无监督方法，无监督方法典型的是聚类分析。
k通常是一个小于10的数字，
k的选择很有艺术性，可能不存在唯一解，来回迭代，无法收敛。而且结论可能很难给出合理解释
***
#CH4
如果想筛选垃圾邮件，为何不能考虑线性模型和KNN分类模型呢？
**线性模型为啥不行**：第一，垃圾邮件是0，1问题，而线性模型的输出是连续值。其次，样本量导致特征（单词）数量会非常大，最小二乘的矩阵不可逆，变量太多不可控。
tips：SGD解决不可逆问题，+逻辑回归解决0，1问题，而且这种方法能考虑单词变量间的相关性问题。但是朴素贝叶斯太简单好用稳定了。。。

**KNN为啥不行**：考虑两封邮件的包含的单词十分相似，但是变量个数太多，维度太高，高维空间中，两个邻居也会相邻甚远，这就是高维诅咒。
**手写数字** 可以用KNN来解决，16*16的像素，拉长成为一个256的向量，计算欧式距离，用混淆矩阵来评价模型。

**朴素贝叶斯**：朴素主要指代的是独立性
垃圾邮件概率：
P(spam/word) = p(word/spam) * p(spam)/P(word)
步骤：计算垃圾邮件比例，正常邮件比例，两种邮件同一个单词的出现概率，就可以算出一个单词出现后，垃圾邮件的概率
这只是单个单词的算法，一封邮件有很多单词，依靠独立性假设，可以连乘，如果连乘项中太多0，会影响计算稳定性，所以一般取对数log。
对数变形后的朴素贝叶斯和线性回归模型张的很相似，唯一不同之处在于，对于参数，用的是贝叶斯公式，而不是计算逆矩阵。
朴素贝叶斯是一个线性模型（在这里是一个线性分类器）
**拉普拉斯平滑**
主要是为了解决邮件中新加入词汇，零概率的问题
如果独立性假设成立，朴素贝叶斯中的θ的估计值恰好是它的最大似然估计值。
如果加入先验信息P（θ）（意思就是对某个单词出现的预先估计），结合似然函数就可以得到最大后验似然估计（MAP）
核心就是，假设θ服从一个Beta分布（α，β），那么θ的最大后验估计量就是拉普拉斯平滑值
α，β取值很讲究，都趋近于0和太集中都不是很好，折中的办法是设定为一个很小的正值。
对于先验信息分布，样本量越大越可以放松假设，因为大样本量的似然函数会掩盖掉先验的影响。
***
#CH5
分类器也就是分类模型，给定数据，输出数据对应某个类别或者是某些类别的概率值。
先讨论最简单的二元分类器，包括，逻辑回归，决策树，随机深林，支持向量机，神经网络等等
问题：
1. 使用哪个分类器
2. 应用何种方法优化训练分类器
3. 选择什么样的损失函数
4. 那些特征变量对建模有用
5. 如何评价模型的实际效果

模型选择的限制条件：运行时间，理解能力，可解释性，可扩展能力
模型的复杂程度与精度成正比，没有万能算法，只有合适的算法

M6D的逻辑回归案例：
要解决三个核心问题
1. 特征工程，找出最有用的特征变量
 2. 交互预测，要有迅速的用户相应能力
 3. 精准定价，给用户展示产生的价值是多少
每个用户的点击网址做成矩阵，行是客户，列是网站，会是一个非常庞大的稀疏矩阵，有大量的0存在。
我们用一个分类变量作为被解释变量，比如某一条广告是否被点击，分为0，1
这个训练数据集就算构成了，此时当然可以考虑朴素贝叶斯，因为同样适用，但是最佳的模型是逻辑回归，因为它本身输出就是0，1直接的概率值
逻辑回归的最终形式可以巧妙的转化为线性模型的形式，而模型的输出结果被限制在了0，1之间。对于点击模型来说，用户点击的逻辑概率可以用该用户的网页访问历史的特征变量（就是稀疏矩阵）的线性组合来表示。
模型中的α表示基准值，也就是一无所知情况下的猜测，所以通常很小，代表无条件概率。
模型拟合，α和β的参数估计，因为逻辑回归中的似然函数的最大化问题没有解析解，不能通过传统求导方式来解决，但是似然函数是一个凸函数，所以可以用凸优化的办法来做。
先取对数，然后加上负号，最大化变成了一个最小化问题，凸优化来找最小化负数对数似然函数的最优解，也就是α和β。有两种常用方法，通常都可以收敛到全局最优解，这里正常情况是指变量相互独立，从而保证优化过程中的海塞矩阵满足正定性条件。
牛顿法：很少直接对海塞矩阵求逆，而是通过解一个类似Ax=y的线性方程组间接的找到A的逆
梯度下降法：不需要求逆，有效应用与大数据和稀疏矩阵的情况，优化效果不是很好，依赖于步幅设定。
模型评价：不同问题采用不同标准
如果对输出类别的相对排序感兴趣（而不是个别类的绝对概率值）那么适用的模型评价方法两个
1，ROC曲线
2，累积提升图
如果用于分类问题：
1，提升度 2准确度 3 精准度 4召回率 5 F得分
如果想评价或比较模型输出的实际概率值的精确程度：
1，均方误差 2 根均方误差 3 平均绝对离差
AUC比提升曲线更适合用作模型比较，AB测试我们真正想要的那个评价指标。
***
#CH7
特征选择
原始数据存在大量冗余，很多相关性高的变量。这些变量需要挑选，转化，组合，比如取对数，连续变离散，往往能提高模型的预测能力。过多的特征会产生稀疏性问题。
可以开展头脑风暴，进行特征提取。
特征选择有篇非常好的文章可以读，P149
主流的变量选择有三种，过滤型，打包型，内嵌型
过滤型：根据特征变量和因变量之间的某个统计量的值，从大到小排列出来，过滤掉一批，可以用简单相关系数，参数估计的p值，模型拟合的R方，但是可能某些变量单个不重要，放在一起比较重要。
这个方法没考虑变量间的相互作用。
包装型：把N个变量，打包成子集，子集的大小是固定的K，也就是N选K
**两个细节：**
1. 选择什么样的标准衡量变量或者变量子集的优劣 2 选择什么样的算法选择最优子集
什么算法合适？
先讨论一下分步回归（我记得还有岭回归，blabla。。。
方法特点：对于回归模型，变量用一种系统性的方式放入或移除模型中，同时用类似R方这样的选择标准记录某个变量被放入模型或移出模型对整个模型效果的影响。
向前搜索法
           先建立一个没有任何变量的回归模型，每一步都从变量集合中选一个加入模型，选择标准就是最大程度能提高拟合或者预测效果，变量加入就不会移除掉，直到剩下的变量加入再也不能提升模型
2.  向后消除法
    与上面方法正好相反，每次消除一个，直到不能再提升
3.  双向搜索法
就是前两种的结合

选择标准
R方，P值，AIC，BIC（都是越小越好），熵

决策树
决策树是非常好用的，容易理解，导向明确，结构清晰的分类方法
唯一问题是如何有效的构建决策树，让变量合理的放在树上。
不是瞎猜，用的是信息量最大的变量，如何认定信息量大小呢？ 熵！！！
在信息论里面，熵用来定量表示一个特征变量所包含的信息量。
一个变量不确定性越高，其熵值越大
把熵作为目标函数，最优化模型的参数，定义信息增益的概念IG（information gain）
搭建决策树时候，总是先放IG大的变量
一个完整迭代的决策树往往会过拟合，为了解决这个问题，可以采用“事后剪枝法”
决策树模型隐藏了一个变量选择的过程，称为“嵌入型变量选择”，因为采用熵的过程，已经自动选择了对模型本身而言最为重要的变量，选择过程嵌入了决策树的搭建过程。
随机森林是决策树模型的拓展，它基于bagging模型，bagging模型的全称是bootstrap aggregating。这个办法可以显著的提高模型精度和稳健性，问题是牺牲了模型的最大优点：可解释性。
bootstraping是一种可放回的抽样方式，抽样样本n,通常是总数据量的80%
N代表森林中决策树的个数，F代表每棵树上使用的特征变量个数
建立随机森林模型两部：1.从原始数据中抽取N个不同bootstrap样本，每个样本建立一个决策树，每个决策树只随机使用F个特征变量
2每个决策树搭建遵循之前的原则
***
#CH10
回顾最邻近算法KNN，严重依赖于对于距离的定义，对于用户的喜好变量可以用jaccard距离来测度用户之间喜好的相似程度，另外余弦度和欧式距离也是合理的选择
KNN面临的问题：1 维度诅咒 2 过拟合 3多重相关问题 4 特征变量之间的相对重要性 5 稀疏性
6 测量误差 7 计算的复杂度 8 距离测度指标的敏感性 9 时变效应 10 模型更新成本
维度诅咒和过拟合问题最重要
为了解决过拟合问题，可以通过设定贝叶斯先验信息的方式强制所有的参数估计值都落在一定范围之内，从模型形式上来看，我们可以惩罚过大的参数估计值，惩罚大参数等同于在数据的协方差矩阵上加了一个先验信息矩阵。最后的模型解只取决于一个参数，通常用λ来表示
剩下的问题是如何选择惩罚参数λ，可以用CH3中的模型交叉验证的方法，不停变换λ的值，直到模型的评价标准告诉你已经找到了一个较为合理的值。只要λ够大，模型一般都会有唯一的最优解，但是过大，会导致全部参数接近0，无意义。
高维度问题：解决该问题的两大杀手锏，PCA主成分分析，和SVD奇异值分解
通过隐含变量（latent feature）的方式达到降维的目的
推荐系统中，问卷中多设计一些比对问题（comparison question）能够带来更好的推荐效果。
SVD（奇异值分解）：X=USV
U和V是square unitary matrix ,而S被称为rectangular matrix  我们通过减秩的方法尽可能的近似矩阵X，同时达到降维的目的。X是原始数据包含了客户和商品信息，X的秩为k，k也是X中可能包含的隐含变量的个数d的上限，d是SVD需要确定的调整参数。
U代表用户，V的行代表商品，S矩阵对角线上的元素叫做奇异值，最重要的隐含标量对应最大的奇异值。
SVD有两大缺陷，缺失值和计算量大两个问题。
PCA：保留U和V，抛弃S矩阵，则有 X 近似= UV，这是个最优化问题，最小化它们的误差平方和。
也就是说想到找到一组最优的U和V矩阵，以最小化预测值与实际观察值之间的平方误差。
这里唯一需要确定的参数是d，它代表了隐含变量的个数，矩阵U的行代表用户，列代表隐含变量，V的行代表商品，列代表隐含变量。
隐含变量是互不相关的，
交替最小二乘法：上述过程似乎是先最小化误差平方和，在通过最小化矩阵UheV的元素平方和才能找到最佳的矩阵U和V。但其实可以两步同时完成。
没有解析解，只能用梯度下降法，只要这个最优化对象是个凸函数，那么就基本能保证收敛到真值，即便函数本身不是一个凸函数，我们也可以通过增加惩罚项的方式把函数强制的变为一个凸函数。
详细步骤：1随机生成一个矩阵V 2 固定矩阵V，最优化V 3 固定矩阵U，最优化U， 4 重复直到UV不再显著的变动为止，可以定义一个 ε作为容忍值，不超过它就可以算收敛。
只要先验信息足够，上面的迭代算法一定收敛。
